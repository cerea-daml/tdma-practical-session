{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <span style=\"color:teal\"> Introduction to surrogate modelling in the geosciences </span>\n",
    "\n",
    "#### Marc Bocquet¹ [marc.bocquet@enpc.fr](mailto:marc.bocquet@enpc.fr) and Alban Farchi¹ [alban.farchi@enpc.fr](mailto:alban.farchi@enpc.fr)\n",
    "##### (1) CEREA, École des Ponts and EdF R&D, IPSL, Île-de-France, France\n",
    "\n",
    "During this session, we will apply standard machine learning methods to learn the dynamics of the Lorenz 1996 model. The objective here is to get a preview of how machine learning can be applied to geoscientific models in a low-order models where testing is quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Importing all modules and define some visualisation functions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import toolbox\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> I. The Lorenz 1996 model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lorenz 1996 (L96, [Lorenz and Emanuel 1998](https://journals.ametsoc.org/view/journals/atsc/55/3/1520-0469_1998_055_0399_osfswo_2.0.co_2.xml)) is a low-order chaotic model commonly used in data assimilation to asses the performance of new algorithms. It represents the evolution of some unspecified scalar meteorological quantity (perhaps vorticity or temperature) over a latitude circle.\n",
    "\n",
    "The model **dynamics** is driven by the following set of ordinary differential equations (ODEs):\n",
    "$$\n",
    "    \\forall n \\in [1, N_{x}], \\quad \\frac{\\mathrm{d}x_{n}}{\\mathrm{d}t} =\n",
    "    (x_{n+1}-x_{n-2})x_{n-1}-x_{n}+F,\n",
    "$$\n",
    "where the indices are periodic: $x_{-1}=x_{N_{x}-1}$, $x_{0}=x_{N_{x}}$, and $x_{1}=x_{N_{x}+1}$, and where the system size $N_{x}$ can take arbitrary values.\n",
    "\n",
    "In the standard configuration, $N_{x}=40$ and the forcing coefficient is $F=8$. The ODEs are integrated using a fourth-order Runge-Kutta scheme with a time step of $0.05$ model time unit (MTU). The resulting dynamics is **chaotic** with a doubling time of errors around $0.42$ MTU (the corresponding Lyapunov is hence $0.61$ MTU). For comparison, $0.05$ MTU represent six hours of real time and correspond to an average autocorrelation around $0.967$. Finally, the model variability (spatial average of the time standard deviation per variable) is $3.64$.\n",
    "\n",
    "In this series of experiments, we will try to emulate the dynamics of the L96 model using artificial neural networks (NN).\n",
    "1. We start by running the **true model** to build a training dataset.\n",
    "2. We build and **train neural networks** using this dataset.\n",
    "3. We evaluate the **forecast skill** of the surrogate models (the NNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> II. The true model dynamics </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before building the training dataset, let us illustrate the model dynamics.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> II.1. </span> Implement the `tendency()` method of the `Lorenz1996Model` class.\n",
    "  This method should compute the model tendencies. You may use the\n",
    "  [`roll()`](https://numpy.org/doc/stable/reference/generated/numpy.roll.html)\n",
    "  function of `numpy`.\n",
    "- <span style=\"color:blue\"> II.1. </span> Implement the `forward()` method of the `Lorenz1996Model` class.\n",
    "  This method should compute an integration step forward in time.\n",
    "  The Runge--Kutta scheme is explained in the method's docstring.\n",
    "  A simple straightforward implementation with six statements is more \n",
    "  than enough for the present set of experiments.\n",
    "- <span style=\"color:blue\"> II.2. </span> Implement the true model integration in the `perform_true_model_integration()` function.\n",
    "  A simple implementation with a `for-loop` should do the job.\n",
    "- <span style=\"color:blue\"> II.2. </span> Describe the model evolution in the first few time steps and in the long-term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> II.1. Defining the true model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In the following cells, we define the true Lorenz 1996 model using standard values: \n",
    "- the number of variables $N_{x}$ is set to `Nx=40`;\n",
    "- the forcing coefficient $F$ is set to `F=8`;\n",
    "- the integration time step is set to `dt=0.05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lorenz1996Model:\n",
    "    \"\"\"Implementation of the Lorenz 1996 model.\n",
    "    \n",
    "    Use the `tendency()` method to compute the model tendencies (i.e., dx/dt)\n",
    "    and use the `forward()` method to apply an integration step forward in time,\n",
    "    using a fourth order Runge--Kutta scheme.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Nx : int\n",
    "        The number of variables in the model.\n",
    "    F : float\n",
    "        The model forcing.\n",
    "    dt : float\n",
    "        The model integration time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Nx, F, dt):\n",
    "        \"\"\"Initialise the model.\"\"\"\n",
    "        self.Nx = Nx\n",
    "        self.F = F\n",
    "        self.dt = dt\n",
    "\n",
    "    def tendency(self, x):\n",
    "        \"\"\"Compute the model tendencies dx/dt.\n",
    "        \n",
    "        The tendencies are computed by batch using\n",
    "        `numpy` vectorisation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (..., Nx)\n",
    "            Batch of input states.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dx_dt : np.ndarray, shape (..., Nx)\n",
    "            Model tendencies computed at the input states.\n",
    "        \"\"\"\n",
    "        # TODO: implement it!\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply an integration step forward in time.\n",
    "        \n",
    "        This method uses a fourth-order Runge--Kutta scheme:\n",
    "        k1 <- dx/dt at x\n",
    "        k2 <- dx/dt at x + dt/2*k1\n",
    "        k3 <- dx/dt at x + dt/2*k2\n",
    "        k4 <- dx/dt at x + dt*k3\n",
    "        k <- (k1 + 2*k2 + 2*k3 + k4)/6\n",
    "        x <- x + dt*k\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (..., Nx)\n",
    "            Batch of input states.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        integrated_x : np.ndarray, shape (..., Nx)\n",
    "            The integrated states after one step.\n",
    "        \"\"\"\n",
    "        # TODO: implement it!\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "true_model = Lorenz1996Model(Nx=40, dt=0.05, F=8)\n",
    "\n",
    "# save some statistics about the model\n",
    "true_model.model_var = 3.64\n",
    "true_model.doubling_time = 0.42\n",
    "true_model.lyap_time = 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> II.2. Short model integration </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In the following cells, we perform a rather short model integration, in order to illustrate the model dynamics. The initial condition is a random field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_true_model_integration(Nt, Ne=1, seed=None):\n",
    "    \"\"\"Perform an integration in time using the true model.\n",
    "    \n",
    "    The initial state is a batch of random fields.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Nt : int\n",
    "        The number of integration steps to perform.\n",
    "    Ne : int\n",
    "        The batch size.\n",
    "    seed : int\n",
    "        The random seed for the initialisation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xr : np.ndarray, shape (Nt+1, Ne, Nx)\n",
    "        The integrated batch of trajectories.\n",
    "    \"\"\"\n",
    "    # define rng\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # allocate memory\n",
    "    xt = np.zeros((Nt+1, Ne, true_model.Nx))\n",
    "\n",
    "    # initialisation\n",
    "    xt[0] = rng.normal(loc=3, scale=1, size=(Ne, true_model.Nx))\n",
    "    \n",
    "    # TODO: implement the model integration for Nt steps\n",
    "    \n",
    "    # return the trajectory\n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for visualisation purpose\n",
    "xt_plot = perform_true_model_integration(Nt=400, Ne=1, seed=314)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trajectory\n",
    "toolbox.plot_l96_traj(\n",
    "    xt_plot, \n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> III. Prepare the dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we prepare the dataset for the entire set of experiments.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> III.2. </span> Implement the `extract_input_output()` function, in which the \n",
    "  neural network input and output are extracted from a given\n",
    "  trajectory. Use `numpy` slicing for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> III.1. A long model integration for the training data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We now use a true model trajectory to make the **training dataset**. This trajectory starts from a random field (different than the one used for the plotting trajectory) and we discard the first $100$ time steps to get rid of the spin-up process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long model integration for the training data\n",
    "xt_train = perform_true_model_integration(Nt=10_000+100, Ne=1, seed=315)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_train = xt_train[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> III.2. Preprocess the training data </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The training dataset is made of input/output pairs, where the input is the state at a given time, and the output is the state at the following time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_input_output(xt):\n",
    "    # TODO: extract x (input)\n",
    "    # TODO: extract y (output)\n",
    "    # return input/output\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract input/output from the training data\n",
    "x_train, y_train = extract_input_output(xt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalisation using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute input/output mean/std\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "x_std = x_train.std()\n",
    "y_std = y_train.std()\n",
    "\n",
    "# define normalisation/denormalisation functions\n",
    "def normalise_x(x):\n",
    "    return (x - x_mean)/x_std\n",
    "def normalise_y(y):\n",
    "    return (y - y_mean)/y_std\n",
    "def denormalise_x(x_norm):\n",
    "    return x_norm*x_std + x_mean\n",
    "def denormalise_y(y_norm):\n",
    "    return y_norm*y_std + y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training data is normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the training data\n",
    "x_train_norm = normalise_x(x_train)\n",
    "y_train_norm = normalise_y(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> III.3. Shorter model integrations for the validation and testing data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We repeat the same process to make the **validation** and **testing** data. In this case, the trajectory starts from two other random fields (and we still get rid of the spin-up processes) and can be somewhat shorter, but the normalisation must be the same as for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for the validation data\n",
    "xt_valid = perform_true_model_integration(Nt=1_000+100, Ne=1, seed=316)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_valid = xt_valid[100:]\n",
    "\n",
    "# extract input/output from the validation data\n",
    "x_valid, y_valid = extract_input_output(xt_valid)\n",
    "\n",
    "# normalise the validation data\n",
    "x_valid_norm = normalise_x(x_valid)\n",
    "y_valid_norm = normalise_y(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for the testing data\n",
    "xt_test = perform_true_model_integration(Nt=1_000+100, Ne=1, seed=317)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_test = xt_test[100:]\n",
    "\n",
    "# extract input/output from the testing data\n",
    "x_test, y_test = extract_input_output(xt_test)\n",
    "\n",
    "# normalise the testing data\n",
    "x_test_norm = normalise_x(x_test)\n",
    "y_test_norm = normalise_y(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> III.4. An ensemble model integration for the forecast skill data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In order to assess the forecast skill of the surrogate model, we will use a different test dataset, in which we record an ensemble of **trajectories** (instead of an ensemble of input/output pairs). This will allow us to measure the accuracy of the forecast for longer integration times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble integration for the forecast skill data\n",
    "xt_fs = perform_true_model_integration(Nt=400+100, Ne=512, seed=318)\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_fs = xt_fs[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> IV. The baseline model: persistence </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this first test series, we use **persistence** as surrogate model. This will provide baselines for our results later on. Persistence is defined as the model for which there is no time evolution.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> IV.3 & IV.4. </span> Comment the evolution of the forecast errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> IV.1. Evaluate the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We evaluate the model using two metrics: the test mean-squared error (MSE) and the forecast skill.\n",
    "\n",
    "The MSE is the loss function that we will use later to train out surrogate models. The test MSE measures the accuracy of the surrogate model over one iteration, i.e. exactly what it has been trained to do, but using unseen data (the \"test data\" here). Therefore, the test MSE can be used to validate the efficiency of the learning/training process. NB: the test MSE of persistence is a number whose absolute value is not that important per se (because the input and output data have been normalised) but it will be useful to normalise the test MSE of our trained NNs.\n",
    "\n",
    "The forecast skill measures the accuracy of the surrogate models, averaged over an ensemble of unseen trajectories (the \"forecast skill data\" here), i.e. for more than one iteration. This is tipycally the kind of tasks we are interested in. However, one must keep in mind that the surrogate models are (in general) not trained to do that, which means that there is no guarantee of the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_baseline = np.mean(np.square(y_test_norm - x_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "fs_baseline = np.sqrt(np.mean(np.square(xt_fs-xt_fs[0]), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> IV.2. Print the test MSE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we show the value of the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence = {test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> IV.3. Show an example of surrogate model integration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In the following cell, we show one example of model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "toolbox.plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    np.broadcast_to(xt_fs[0, 0], shape=xt_fs[:, 0].shape),\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> IV.4. Show the forecast skill</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we plot the average forecast skill, normalised by the model variability. The shadow delimits the 90% confidence interval (percentiles 5 and 95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "toolbox.plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=4,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> V. A dense neural network as surrogate model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second test series, we train and evaluate a dense NN (sequential NN with only dense layers). \n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> V.1. </span> Implement the `make_sequential_network()` function, in which a \n",
    "  sequential neural network is created. The neural network should\n",
    "  take as input the current state and return the forecasted state.\n",
    "- <span style=\"color:blue\"> V.1. </span> Comment the number of parameters of the built network.\n",
    "- <span style=\"color:blue\"> V.3. </span> Implement the `compute_forecast_skill()` function, in which we \n",
    "  use a surrogate model to predict the trajectories and then\n",
    "  compute the forecast skill. Use the `predict()` method of\n",
    "  `tf.keras.Model` inside a `for-loop`.\n",
    "- <span style=\"color:blue\"> V.4. & V.5 </span> Comment the evolution of the training and validation MSE, as well as the test MSE.\n",
    "- <span style=\"color:blue\"> V.6. & V.7 </span> Comment the evolution of the forecast errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> V.1. Build the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following cells, we build the surrogate model, using the [sequential API of tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "For this example, we use $4$ internal layers and $128$ nodes per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_network(seed, num_layers, num_nodes, activation):\n",
    "    \"\"\"Build a sequential neural network using dense layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_layers : int\n",
    "        The number of hidden layers.\n",
    "    num_nodes : int\n",
    "        The number of nodes per hidden layer.\n",
    "    activation : str\n",
    "        The activation function for the hidden layers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    network : tf.keras.Sequential\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # TODO: create a sequential network\n",
    "    network = ...\n",
    "    # TODO: add the input layer\n",
    "    # TODO: add the hidden layers\n",
    "    # TODO: add the output layer\n",
    "    # compile the neural network\n",
    "    network.compile(loss='mse', optimizer='adam')\n",
    "    # print short summary\n",
    "    network.summary()\n",
    "    # return the network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the dense neural network\n",
    "dense_network = make_dense_network(seed=319, num_layers=4, num_nodes=128, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> V.2. Train the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we train the model for $256$ epochs. We use an EarlyStopping callback to end the training when the validation loss stops improving. This should avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(seed, num_epochs, description, patience, network):\n",
    "    \"\"\"Train a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_epochs : int\n",
    "        The number of epochs.\n",
    "    description : str\n",
    "        The progress bar description.\n",
    "    patience : int\n",
    "        The patience for EarlyStopping.\n",
    "    network : tf.keras.Model\n",
    "        The network to train.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history : dict\n",
    "        The training history.\n",
    "    \"\"\"\n",
    "    # set random seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # tqdm callback\n",
    "    tqdm_callback = toolbox.TQDMCallback(description)\n",
    "    # early stopping callback\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        verbose=0,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    # train the ML model\n",
    "    fit = network.fit(\n",
    "        x_train_norm, \n",
    "        y_train_norm,\n",
    "        verbose=0,\n",
    "        epochs=num_epochs, \n",
    "        validation_data=(x_valid_norm, y_valid_norm),\n",
    "        callbacks=[tqdm_callback, early_stopping_callback],\n",
    "    )\n",
    "    # return training history\n",
    "    return fit.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "fit_dense = train_network(\n",
    "    seed=320, \n",
    "    num_epochs=256, \n",
    "    description='DNN training', \n",
    "    patience=16, \n",
    "    network=dense_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> V.3. Evaluate the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectories(network):\n",
    "    \"\"\"Compute the forecast skill trajectories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : tf.keras.Model\n",
    "        The model to evaluate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xt : np.ndarray, shape (Nt, Ne, Nx)\n",
    "        The trajectories.\n",
    "    \"\"\"\n",
    "    # allocate memory\n",
    "    (Nt, Ne, Nx) = xt_fs.shape\n",
    "    xt = np.zeros((Nt, Ne, Nx))\n",
    "    \n",
    "    # initialisation\n",
    "    xt[0] = xt_fs[0]\n",
    "    \n",
    "    # TODO: implement the neural network integration\n",
    "        \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_dense = dense_network.evaluate(x_test_norm, y_test_norm, verbose=0, batch_size=x_test_norm.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "xt_dense = compute_trajectories(dense_network)\n",
    "fs_dense = np.sqrt(np.mean(np.square(xt_fs-xt_dense), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> V.4. Show the training history</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we plot the training history, that is, the evolution of the training MSE (the `loss`) and the validation MSE (the `val_loss`) as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning history\n",
    "toolbox.plot_learning_curve(\n",
    "    fit_dense['loss'],\n",
    "    fit_dense['val_loss'],\n",
    "    title='DNN training',\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> V.5. Print the test MSE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence  = {test_mse_baseline}')\n",
    "print(f'test mse of DNN          = {test_mse_dense}')\n",
    "print()\n",
    "print(f'relative test mse of DNN = {test_mse_dense/test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> V.6. Show an example of surrogate model integration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "toolbox.plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    xt_dense[:, 0],\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> V.7. Show the forecast skill</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "toolbox.plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "        DNN=fs_dense,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=4,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> VI. A convolutional neural network as surrogate model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third test series, we train and evaluate a convolutional NN (sequential NN with only convolutional layers). \n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> VI.1. </span> Implement the `make_convolutional_network()` function, in which a \n",
    "  sequential neural network with convolutional layers is created. Do not forget to add periodic padding layers\n",
    "  where needed.\n",
    "- <span style=\"color:blue\"> VI.1. </span> Comment the number of parameters of the built network.\n",
    "- <span style=\"color:blue\"> VI.4. & VI.5 </span> Comment the evolution of the training and validation MSE, as well as the test MSE.\n",
    "- <span style=\"color:blue\"> VI.6. & VI.7 </span> Comment the evolution of the forecast errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VI.1. Build the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following cells, we build the surrogate model.\n",
    "\n",
    "For this example, we use $4$ internal layers and $8$ convolutional filters per layer. The kernel size is set to $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_convolutional_network(seed, num_layers, num_filters, kernel_size, activation):\n",
    "    \"\"\"Build a sequential neural network with convolutional layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_layers : int\n",
    "        The number of hidden layers.\n",
    "    num_filters : int\n",
    "        The number of convolution filters per hidden layer.\n",
    "    kernel_size : int\n",
    "        The convolution kernel size for the hidden layer.\n",
    "    activation : str\n",
    "        The activation function for the hidden layers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    network : tf.keras.Sequential\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # reshape layers\n",
    "    reshape_input = tf.keras.layers.Reshape((true_model.Nx, 1))\n",
    "    reshape_output = tf.keras.layers.Reshape((true_model.Nx,))\n",
    "    # padding layer\n",
    "    border = kernel_size//2\n",
    "    def apply_padding(x):\n",
    "        x_left = x[..., -border:, :]\n",
    "        x_right = x[..., :border, :]\n",
    "        return tf.concat([x_left, x, x_right], axis=-2)\n",
    "    padding_layer = tf.keras.layers.Lambda(apply_padding)   \n",
    "    # TODO: create a sequential network\n",
    "    network = ...\n",
    "    # TODO: add the input layer\n",
    "    # TODO: add the reshape_input layer\n",
    "    # TODO: add the hidden layers\n",
    "    # TODO: add the output layer\n",
    "    # TODO: add the reshape_output layer\n",
    "    # compile the neural network\n",
    "    network.compile(loss='mse', optimizer='adam')\n",
    "    # print short summary\n",
    "    network.summary()\n",
    "    # return the network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the conv neural network\n",
    "conv_network = make_convolutional_network(seed=321, num_layers=4, num_filters=8, kernel_size=5, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VI.2. Train the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "fit_conv = train_network(\n",
    "    seed=322, \n",
    "    num_epochs=256, \n",
    "    description='CNN training', \n",
    "    patience=8, \n",
    "    network=conv_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VI.3. Evaluate the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_conv = conv_network.evaluate(x_test_norm, y_test_norm, verbose=0, batch_size=x_test_norm.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "xt_conv = compute_trajectories(conv_network)\n",
    "fs_conv = np.sqrt(np.mean(np.square(xt_fs-xt_conv), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VI.4. Show the training history</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning history\n",
    "toolbox.plot_learning_curve(\n",
    "    fit_conv['loss'],\n",
    "    fit_conv['val_loss'],\n",
    "    title='CNN training',\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VI.5. Print the test MSE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence  = {test_mse_baseline}')\n",
    "print(f'test mse of DNN          = {test_mse_dense}')\n",
    "print(f'test mse of CNN          = {test_mse_conv}')\n",
    "print()\n",
    "print(f'relative test mse of DNN = {test_mse_dense/test_mse_baseline}')\n",
    "print(f'relative test mse of CNN = {test_mse_conv/test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VI.6. Show an example of surrogate model integration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "toolbox.plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    xt_conv[:, 0],\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VI.7. Show the forecast skill</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "toolbox.plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "        DNN=fs_dense,\n",
    "        CNN=fs_conv,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=10,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> VII. A smart neural network as surrogate model </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third and last test series, we train and evaluate a smart NN. This NN uses a sparse architecture with convolutional NN and controlled nonlinearity to reproduce the **model tendencies**, as well as a Runge-Kutta integration scheme to **emulate the dynamics**. In order to implement this NN, we use both the [functional API](https://www.tensorflow.org/guide/keras/functional) (for the model tendency) and the [subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) (for the integration scheme) of tensorflow.\n",
    "\n",
    "In this case, with well-chosen parameters it is possible to reproduce the true dynamics up to machine precision: the model is said to be identifiable.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- <span style=\"color:blue\"> VII.1. </span> Comment the number of parameters of the built network.\n",
    "- <span style=\"color:blue\"> VII.4. & VII.5 </span> Comment the evolution of the training and validation MSE, as well as the test MSE.\n",
    "- <span style=\"color:blue\"> VII.6. & VII.7 </span> Comment the evolution of the forecast errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VII.1. Build the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartNetwork(tf.keras.Model):\n",
    "    \"\"\"Smart neural network for the Lorenz 1996 model.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dt : float\n",
    "        The integration time step.\n",
    "    tendency : tf.keras.Model\n",
    "        The network to compute the tendencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters, kernel_size, dt=0.05, **kwargs):\n",
    "        \"\"\"Initialise the smart network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_filters : int\n",
    "            Number of filters to use in the convolutional layer.\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel.\n",
    "        dt : float\n",
    "            Integration time step.\n",
    "        kwargs : dict\n",
    "            Additional parameters forwarded to `tf.keras.Model.__init__()`.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dt = dt\n",
    "        \n",
    "        # reshape layers\n",
    "        reshape_input = tf.keras.layers.Reshape((true_model.Nx, 1))\n",
    "        reshape_output = tf.keras.layers.Reshape((true_model.Nx,))\n",
    "        \n",
    "        # padding layer\n",
    "        border = kernel_size//2\n",
    "        def apply_padding(x):\n",
    "            x_left = x[..., -border:, :]\n",
    "            x_right = x[..., :border, :]\n",
    "            return tf.concat([x_left, x, x_right], axis=-2)\n",
    "        padding_layer = tf.keras.layers.Lambda(apply_padding)\n",
    "        \n",
    "        # convolutional layers\n",
    "        conv_layer_1 = tf.keras.layers.Conv1D(num_filters, kernel_size)\n",
    "        conv_layer_2 = tf.keras.layers.Conv1D(1, 1)\n",
    "        \n",
    "        # network for the model tendencies\n",
    "        x_in = tf.keras.Input(shape=(true_model.Nx,))\n",
    "        # reshape the input to be able to use convolutional layers\n",
    "        x = reshape_input(x_in)\n",
    "        # apply convolution with periodic padding\n",
    "        x = padding_layer(x)\n",
    "        x1 = conv_layer_1(x)\n",
    "        # construct non-linear terms\n",
    "        x2 = x1 * x1\n",
    "        # concatenate linear and non-linear terms\n",
    "        x3 = tf.concat([x1, x2], axis=-1)\n",
    "        # combine all channels into one\n",
    "        # there is no actual convolution here \n",
    "        # because the kernel_size is one for this layer\n",
    "        x_out = conv_layer_2(x3)\n",
    "        # reshape the output after the convolutional layers\n",
    "        x_out = reshape_output(x_out)\n",
    "        # pack everything into a tf.keras.Model\n",
    "        self.tendency = tf.keras.Model(inputs=x_in, outputs=x_out)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        \"\"\"Apply the network.\"\"\"\n",
    "        dx_dt_0 = self.tendency(x)\n",
    "        dx_dt_1 = self.tendency(x+0.5*self.dt*dx_dt_0)\n",
    "        dx_dt_2 = self.tendency(x+0.5*self.dt*dx_dt_1)\n",
    "        dx_dt_3 = self.tendency(x+self.dt*dx_dt_2)\n",
    "        dx_dt =  (dx_dt_0 + 2*dx_dt_1 + 2*dx_dt_2 + dx_dt_3)/6\n",
    "        return x + self.dt*dx_dt\n",
    "    \n",
    "def make_smart_network(seed, num_filters, kernel_size):\n",
    "    \"\"\"Build a sequential neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_filters : int\n",
    "        The number of filters.\n",
    "    kernel_size : int\n",
    "        The convolution kernel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    network : SmartNetwork\n",
    "        The smart network.\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # create the network\n",
    "    network = SmartNetwork(\n",
    "        num_filters=num_filters, \n",
    "        kernel_size=kernel_size, \n",
    "        dt=true_model.dt,\n",
    "    )\n",
    "    # compile the neural network\n",
    "    network.compile(loss='mse', optimizer='adam')\n",
    "    # print short summary\n",
    "    network.tendency.summary()\n",
    "    # return the network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the smart neural network\n",
    "smart_network = make_smart_network(seed=323, num_filters=6, kernel_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VII.2. Train the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "fit_smart = train_network(\n",
    "    seed=324, \n",
    "    num_epochs=128, \n",
    "    description='smart NN training', \n",
    "    patience=8, \n",
    "    network=smart_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VII.3. Evaluate the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_smart = smart_network.evaluate(x_test_norm, y_test_norm, verbose=0, batch_size=x_test_norm.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "xt_smart = compute_trajectories(smart_network)\n",
    "fs_smart = np.sqrt(np.mean(np.square(xt_fs-xt_smart), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VII.4. Show the training history</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning history\n",
    "toolbox.plot_learning_curve(\n",
    "    fit_smart['loss'],\n",
    "    fit_smart['val_loss'],\n",
    "    title='Smart network training',\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VII.5. Print the test MSE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence        = {test_mse_baseline}')\n",
    "print(f'test mse of DNN                = {test_mse_dense}')\n",
    "print(f'test mse of CNN                = {test_mse_conv}')\n",
    "print(f'test mse of smart net          = {test_mse_smart}')\n",
    "print()\n",
    "print(f'relative test mse of DNN       = {test_mse_dense/test_mse_baseline}')\n",
    "print(f'relative test mse of CNN       = {test_mse_conv/test_mse_baseline}')\n",
    "print(f'relative test mse of smart net = {test_mse_smart/test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> VI.6. Show an example of surrogate model integration</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "toolbox.plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    xt_smart[:, 0],\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> VI.7. Show the forecast skill</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "toolbox.plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "        DNN=fs_dense,\n",
    "        CNN=fs_conv,\n",
    "        smart=fs_smart,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=20,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
